{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jufeitosa10/XAI_Segmentacao_Pacu/blob/main/Combinacao_Metodos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZjOtfCDWka4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b17ab84-76dd-45f8-b2cd-226e5ac4d5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ],
      "id": "vZjOtfCDWka4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNtNnxdbgzpp"
      },
      "outputs": [],
      "source": [
        "#para pegar o que tem  no drive e poder usar\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/XAI')"
      ],
      "id": "sNtNnxdbgzpp"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models.detection.anchor_utils\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image, ExifTags, ImageFile\n",
        "from torchvision.transforms import functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "import os\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor, MaskRCNN\n",
        "\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ToTensor(object):\n",
        "    def __call__(self, image, target):\n",
        "        image = F.to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "def get_transform():\n",
        "    transforms = []\n",
        "    transforms.append(ToTensor())\n",
        "\n",
        "    return Compose(transforms)\n",
        "\n",
        "class LabelboxDataset(object):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"masks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"masks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        width,height = img.size\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            if xmin >= 0 and xmax <= width and xmin <= xmax and ymin >= 0  and ymax <= height and ymin <= ymax:\n",
        "                xmin_new = min(xmin, xmax)\n",
        "                xmax_new = max(xmin, xmax)\n",
        "                boxes.append([xmin_new, ymin, xmax_new, ymax])\n",
        "            else:\n",
        "                print(self.imgs[idx])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        labels = labels[keep]\n",
        "        masks = masks[keep]\n",
        "        area = area[keep]\n",
        "        iscrowd = iscrowd[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "ORIENTATION_ROTATION_MAP = {\n",
        "    0: 0,\n",
        "    1: 360,\n",
        "    2: 360,\n",
        "    3: 180,\n",
        "    4: 180,\n",
        "    5: 270,\n",
        "    6: 270,\n",
        "    7: 90,\n",
        "    8: 90,\n",
        "}\n",
        "\n",
        "class URLDataset(object):\n",
        "    def __init__(self, urls, transforms, scale=1.0):\n",
        "        self.urls = urls\n",
        "        self.transforms = transforms\n",
        "        self.scale = scale\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = Image.open(BytesIO(requests.get(self.urls[idx]).content)).convert(\"RGB\")\n",
        "        if image.getexif():\n",
        "            exif = dict((ExifTags.TAGS[k], v) for k, v in image._getexif().items() if k in ExifTags.TAGS)\n",
        "            orient = exif.get('Orientation', 0)\n",
        "            if orient > 0:\n",
        "                image = image.rotate(ORIENTATION_ROTATION_MAP[orient], expand=True)\n",
        "        w,h = image.size\n",
        "        scaled_w = int(np.ceil(w * self.scale))\n",
        "        scaled_h = int(np.ceil(h * self.scale))\n",
        "\n",
        "        image = image.resize((scaled_w, scaled_h), Image.ANTIALIAS)\n",
        "\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.zeros((scaled_h, scaled_w), np.float32)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        width,height = image.size\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            if xmin >= 0 and xmax <= width and xmin <= xmax and ymin >= 0  and ymax <= height and ymin <= ymax:\n",
        "                xmin_new = min(xmin, xmax)\n",
        "                xmax_new = max(xmin, xmax)\n",
        "                boxes.append([xmin_new, ymin, xmax_new, ymax])\n",
        "            else:\n",
        "                print(self.urls[idx])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        if len(boxes):\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        else:\n",
        "            area = 0\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "#         keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0])\n",
        "#         boxes = boxes[keep]\n",
        "#         labels = labels[keep]\n",
        "#         masks = masks[keep]\n",
        "#         area = area[keep]\n",
        "#         iscrowd = iscrowd[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target, self.urls[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urls)\n",
        "\n",
        "class FolderDataset(object):\n",
        "    def __init__(self, urls, transforms, scale=1.0):\n",
        "        self.urls = urls\n",
        "        self.transforms = transforms\n",
        "        self.scale = scale\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.urls[idx]).convert(\"RGB\")\n",
        "        if image.getexif():\n",
        "            exif = dict((ExifTags.TAGS[k], v) for k, v in image.getexif().items() if k in ExifTags.TAGS)\n",
        "            orient = exif.get('Orientation', 0)\n",
        "            if orient > 0:\n",
        "                image = image.rotate(ORIENTATION_ROTATION_MAP[orient], expand=True)\n",
        "        w,h = image.size\n",
        "        scaled_w = int(np.ceil(w * self.scale))\n",
        "        scaled_h = int(np.ceil(h * self.scale))\n",
        "\n",
        "        image = image.resize((scaled_w, scaled_h), Image.ANTIALIAS)\n",
        "\n",
        "        # convert the PIL Image into a numpy array\n",
        "        mask = np.zeros((scaled_h, scaled_w), np.float32)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        width,height = image.size\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            if xmin >= 0 and xmax <= width and xmin <= xmax and ymin >= 0  and ymax <= height and ymin <= ymax:\n",
        "                xmin_new = min(xmin, xmax)\n",
        "                xmax_new = max(xmin, xmax)\n",
        "                boxes.append([xmin_new, ymin, xmax_new, ymax])\n",
        "            else:\n",
        "                print(self.urls[idx])\n",
        "\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        if len(boxes):\n",
        "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        else:\n",
        "            area = 0\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "#         keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0])\n",
        "#         boxes = boxes[keep]\n",
        "#         labels = labels[keep]\n",
        "#         masks = masks[keep]\n",
        "#         area = area[keep]\n",
        "#         iscrowd = iscrowd[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image, target = self.transforms(image, target)\n",
        "\n",
        "        return image, target, self.urls[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.urls)\n",
        "def get_maskrcnn_model(num_classes, FROZEN_BLOCKS):\n",
        "    resnet = torchvision.models.resnet18(pretrained=True)\n",
        "    backbone = torch.nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, resnet.layer1,\n",
        "                                  resnet.layer2, resnet.layer3, resnet.layer4)\n",
        "    # Fix blocks\n",
        "    for p in backbone[0].parameters():\n",
        "        p.requires_grad = False\n",
        "    for p in backbone[1].parameters():\n",
        "        p.requires_grad = False\n",
        "    if FROZEN_BLOCKS >= 3:\n",
        "        for p in backbone[6].parameters():\n",
        "            p.requires_grad = False\n",
        "    if FROZEN_BLOCKS >= 2:\n",
        "        for p in backbone[5].parameters():\n",
        "            p.requires_grad = False\n",
        "    if FROZEN_BLOCKS >= 1:\n",
        "        for p in backbone[4].parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def set_bn_fix(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('BatchNorm') != -1:\n",
        "            for p in m.parameters(): p.requires_grad = False\n",
        "    backbone.apply(set_bn_fix)\n",
        "\n",
        "    backbone.out_channels = 512\n",
        "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                        aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','3'],\n",
        "                                              output_size=7,\n",
        "                                              sampling_ratio=2)\n",
        "\n",
        "    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','3'],\n",
        "                                                                output_size=14,\n",
        "                                                                sampling_ratio=2)\n",
        "    return MaskRCNN(backbone,\n",
        "                        num_classes=num_classes,\n",
        "                        rpn_anchor_generator=anchor_generator,\n",
        "                        box_roi_pool=roi_pooler,\n",
        "                        box_detections_per_img=10,\n",
        "                        mask_roi_pool=mask_roi_pooler)\n",
        "def get_maskrcnn_mobilenetv2_model(num_classes):\n",
        "    backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "    backbone.out_channels = 1280\n",
        "\n",
        "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                        aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','3'],\n",
        "                                              output_size=7,\n",
        "                                              sampling_ratio=2)\n",
        "\n",
        "    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0','1','2','3'],\n",
        "                                                                output_size=14,\n",
        "                                                                sampling_ratio=2)\n",
        "    return MaskRCNN(backbone,\n",
        "                        num_classes=num_classes,\n",
        "                        rpn_anchor_generator=anchor_generator,\n",
        "                        box_roi_pool=roi_pooler,\n",
        "                        box_detections_per_img=10,\n",
        "                        mask_roi_pool=mask_roi_pooler)\n",
        "def get_coloured_mask(mask, label=None):\n",
        "    \"\"\"\n",
        "    random_colour_masks\n",
        "      parameters:\n",
        "        - image - predicted masks\n",
        "      method:\n",
        "        - the masks of each predicted object is given random colour for visualization\n",
        "    \"\"\"\n",
        "    colours = [\n",
        "        [80, 220, 100], # index 0 is yellow\n",
        "        [255, 0, 0], # index 1 is redtransforms\n",
        "        [0, 64, 255], # index 2 is blue\n",
        "        [0, 64, 255], # index 2 is blue\n",
        "        [255, 0, 255],\n",
        "        [255,255,255], # index\n",
        "        # [157,32,161],\n",
        "        # [4,44,213],\n",
        "    ]\n",
        "    r = np.zeros_like(mask).astype(np.uint8)\n",
        "    g = np.zeros_like(mask).astype(np.uint8)\n",
        "    b = np.zeros_like(mask).astype(np.uint8)\n",
        "    if label is not None:\n",
        "        color = colours[label]\n",
        "    else:\n",
        "        color = colours[random.randrange(0,len(colours))]\n",
        "    r[mask == 1], g[mask == 1], b[mask == 1] = color\n",
        "    coloured_mask = np.stack([r, g, b], axis=2)\n",
        "    return coloured_mask\n",
        "\n",
        "\n",
        "def display_pred(image, prediction, thres=0.50, alpha = 0.50):\n",
        "    image = np.array(image)\n",
        "\n",
        "\n",
        "    labels = prediction['labels'].cpu().clone().detach().numpy()\n",
        "    masks = prediction['masks'].cpu().clone().detach().numpy()\n",
        "    boxes = prediction[\"boxes\"].cpu().clone().detach().numpy()\n",
        "    scores = prediction[\"scores\"].cpu().clone().detach().numpy()\n",
        "    #max_score_idx = scores.argsort()[-top:][::-1]\n",
        "\n",
        "    rect_th=1\n",
        "\n",
        "\n",
        "    for idx,score  in enumerate(scores):\n",
        "        box, mask, label= boxes[idx], np.copy(masks[idx]),labels[idx]\n",
        "        if score >= thres:\n",
        "            mask[np.where(mask>thres)] = 1\n",
        "            mask[np.where(mask<thres)] = 0\n",
        "\n",
        "            rgb_mask = get_coloured_mask(mask, label).squeeze().transpose(0,2,1)\n",
        "\n",
        "            image = cv2.addWeighted(image, 1, rgb_mask, alpha, 0)\n",
        "\n",
        "            start = (box[0], box[3])\n",
        "            end = (box[2],box[1])\n",
        "            cv2.rectangle(image, start, end,color=(255, 0, 0), thickness=rect_th)\n",
        "\n",
        "    return image\n",
        "\n",
        "def display_black_mask(image, prediction, thres=0.50, alpha = 0.50):\n",
        "    image = np.array(image)\n",
        "    zeros = np.zeros(image.shape)\n",
        "    labels = prediction['labels'].cpu().clone().detach().numpy()\n",
        "    masks = prediction['masks'].cpu().clone().detach().numpy()\n",
        "    boxes = prediction[\"boxes\"].cpu().clone().detach().numpy()\n",
        "    scores = prediction[\"scores\"].cpu().clone().detach().numpy()\n",
        "    #max_score_idx = scores.argsort()[-top:][::-1]\n",
        "    rect_th=1\n",
        "    for idx,score  in enumerate(scores):\n",
        "        box, mask, label= boxes[idx], np.copy(masks[idx]),labels[idx]\n",
        "        if score >= thres:\n",
        "            mask[np.where(mask>thres)] = 1\n",
        "            mask[np.where(mask<thres)] = 0\n",
        "            m = np.uint8(mask.transpose(1,2,0)*image)\n",
        "            zeros = cv2.addWeighted(np.uint8(zeros), 1, m, 1, 0)\n",
        "\n",
        "            start = (box[0], box[3])\n",
        "            end = (box[2],box[1])\n",
        "            cv2.rectangle(zeros, start, end,color=(255, 0, 0), thickness=rect_th)\n",
        "\n",
        "    return zeros\n",
        "\n",
        "def display_white_mask(image, prediction, thres=0.50, alpha = 0.50):\n",
        "    image = np.array(image)\n",
        "    zeros = np.zeros(image.shape)\n",
        "    labels = prediction['labels'].cpu().clone().detach().numpy()\n",
        "    masks = prediction['masks'].cpu().clone().detach().numpy()\n",
        "    boxes = prediction[\"boxes\"].cpu().clone().detach().numpy()\n",
        "    scores = prediction[\"scores\"].cpu().clone().detach().numpy()\n",
        "    #max_score_idx = scores.argsort()[-top:][::-1]\n",
        "    rect_th=1\n",
        "    for idx,score  in enumerate(scores):\n",
        "        box, mask, label= boxes[idx], np.copy(masks[idx]),labels[idx]\n",
        "        if score >= thres:\n",
        "            mask[np.where(mask>thres)] = 1\n",
        "            mask[np.where(mask<thres)] = 0\n",
        "            m = np.uint8(image*mask.transpose(1,2,0))\n",
        "            zeros = cv2.addWeighted(np.uint8(zeros), 1.0, m, 1, 0)\n",
        "\n",
        "            start = (box[0], box[3])\n",
        "            end = (box[2],box[1])\n",
        "            #cv2.rectangle(zeros, start, end,color=(255, 0, 0), thickness=rect_th)\n",
        "    zeros[np.where(zeros==0)] = 255\n",
        "    return zeros\n",
        "\n",
        "from PIL import ImageFile\n",
        "import os\n",
        "\n",
        "def check_resolutions(IMAGE_DIR):\n",
        "    ImPar=ImageFile.Parser()\n",
        "\n",
        "    dicta = {}\n",
        "    i = 0\n",
        "    for fi in os.listdir(IMAGE_DIR):\n",
        "        with open(f\"{IMAGE_DIR}/{fi}\", \"rb\") as f:\n",
        "            #print(fi)\n",
        "            if \".csv\" not in fi:\n",
        "                ImPar=ImageFile.Parser()\n",
        "                chunk = f.read(2048)\n",
        "                count=2048\n",
        "                while chunk != \"\" and chunk != None:\n",
        "\n",
        "                    ImPar.feed(chunk)\n",
        "                    if ImPar.image:\n",
        "                        break\n",
        "                    chunk = f.read(2048)\n",
        "                    count+=2048\n",
        "                dicta[ImPar.image.size] = fi\n",
        "                i+=1\n",
        "                #print(i)\n",
        "    return dicta\n"
      ],
      "metadata": {
        "id": "N9pO4pmmNp6F"
      },
      "id": "N9pO4pmmNp6F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84b65d2"
      },
      "source": [
        "#Combinações\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "d84b65d2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a57fbb0",
        "outputId": "24e02a33-5bc8-4b88-af9c-6c924168823e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: grad-cam in /usr/local/lib/python3.10/dist-packages (1.4.8)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.65.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from grad-cam) (4.7.0.72)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (0.15.2+cu118)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from grad-cam) (8.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from grad-cam) (2.0.1+cu118)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from grad-cam) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from grad-cam) (1.22.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->grad-cam) (2.0.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->grad-cam) (16.0.5)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->grad-cam) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.8.2->grad-cam) (2.27.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->grad-cam) (1.4.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->grad-cam) (1.10.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# !pip install pip==21.3.1\n",
        "!pip install grad-cam\n",
        "\n",
        "# Taken from the torchvision tutorial\n",
        "# https://pytorch.org/vision/stable/auto_examples/plot_visualization_utils.html\n",
        "# from prediction_utils import URLDataset, get_maskrcnn_model, get_transform, display_pred, display_black_mask, display_white_mask, get_coloured_mask, check_resolutions\n",
        "from torchvision.transforms import ToPILImage\n",
        "from matplotlib import pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "\n",
        "model = get_maskrcnn_model(num_classes=4, FROZEN_BLOCKS=1)\n",
        "#original\n",
        "#model.load_state_dict(torch.load('/resnet18_checkpoint_355.pth', map_location=torch.device(\"cpu\")))\n",
        "\n",
        "#modificado pela Ju - pega arquivo direto do Drive\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/XAI/resnet18_checkpoint_355.pth', map_location=torch.device(\"cpu\")))\n",
        "\n",
        "model = model.eval()\n",
        "model = model.to(torch.device(\"cpu\"))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "5a57fbb0"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import numpy as np\n",
        "import requests\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PIL import Image, ExifTags, ImageFile\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "arq = open(\"/content/drive/MyDrive/Colab Notebooks/XAI/nomes_imagens_peixes2.txt\")\n",
        "linhas = arq.readlines()\n",
        "\n",
        "for linha in linhas:\n",
        "    print(linha)"
      ],
      "metadata": {
        "id": "tJdZfMA1g6YM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad3f420-d910-4e71-fd93-dff99591df26"
      },
      "id": "tJdZfMA1g6YM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckz4dow5r8eay0z9i9oef91zg\n",
            "\n",
            "ckz4e0jvq8h73109bbpqs0fps\n",
            "\n",
            "ckz4e2n4q8f530z8n9s3ubp7y\n",
            "\n",
            "ckz4e3onq8gmi0z9i5c70ay45\n",
            "\n",
            "ckz4e4vvo8i6i109bgzla6hms\n",
            "\n",
            "ckz4e8ge48hhl0z9ifd9773xk\n",
            "\n",
            "ckz4e9ebq8gg20z8ngztba1ez\n",
            "\n",
            "ckz4e650k8ig1109b97k528nn\n",
            "\n",
            "ckz4e72918ion109b5wbr5raw\n",
            "\n",
            "ckz4ea3cz8jbf109bhn9y68q1\n",
            "\n",
            "ckz4ebk6g8i8u0z9i60n60f6l\n",
            "\n",
            "ckz4edumg8ing0z9ifni9g5t8\n",
            "\n",
            "ckz4efrf38kem109b2057fb9a\n",
            "\n",
            "ckz4egfr48kh7109bah7xf5az\n",
            "\n",
            "ckz4fduq78ntw0z8n630rag87\n",
            "\n",
            "ckz4ff6t28o3e0z8neshr668d\n",
            "\n",
            "ckz4ffyse8o9s0z8n3zgmd73e\n",
            "\n",
            "ckz4fhxu88ool0z8n6rwr1cpm\n",
            "\n",
            "ckz4fiupe8rhx109bh3peeyhc\n",
            "\n",
            "ckz4fzcn28sui0z8nakiacny4\n",
            "\n",
            "ckz4g2msm8tqx0z8n6d9pb3iv\n",
            "\n",
            "ckz4g4h098uag0z8nht9q26i8\n",
            "\n",
            "ckz4g5tsc8um20z8n9i7a5xw6\n",
            "\n",
            "ckz4g6h9v8x8e109b16pefhfo\n",
            "\n",
            "ckz4g7nl08v2x0z8nbuek2vjk\n",
            "\n",
            "ckz4g09s18vbm109bhm9pcd53\n",
            "\n",
            "ckz4g9uib8x7y0z9i2x6dgzf8\n",
            "\n",
            "ckz4g17ra8upt0z9ibj0bbdno\n",
            "\n",
            "ckz4g51wk8wtj109bhzrydupk\n",
            "\n",
            "ckz4g70yq8we70z9idki606ak\n",
            "\n",
            "ckz4g39038vcp0z9i67o96efj\n",
            "\n",
            "ckz4g88978v9h0z8n0bmhf6m2\n",
            "\n",
            "ckz4gb3qw8yne109bd7k39lcl\n",
            "\n",
            "ckz4gbrb08xqo0z9i6tencxws\n",
            "\n",
            "ckz4gceez8xwm0z9ihjmr0f68\n",
            "\n",
            "ckz4gd9eh8y3f0z9i649mbt2l\n",
            "\n",
            "ckz4gds8l8zdg109b2j93eman\n",
            "\n",
            "ckz4gedyg8yf60z9i2yz18o5f\n",
            "\n",
            "ckz4gf4eo8zsy109b73nq9xh9\n",
            "\n",
            "ckz4gfy7f909n109b48iz9b6o\n",
            "\n",
            "ckz4ggtun8z5p0z9i0l5j7dsa\n",
            "\n",
            "ckz4ghiqq8zco0z9i22va9vso\n",
            "\n",
            "ckz4gi8fr90wz109b4554e8e3\n",
            "\n",
            "ckz4gity68yeu0z8n2tl4bvke\n",
            "\n",
            "ckz4gjkpy8zwc0z9ihsd41tus\n",
            "\n",
            "ckz4gk5d08ypo0z8n6p8eear8\n",
            "\n",
            "ckz4gksld91kk109b5t1y78vp\n",
            "\n",
            "ckz4glf1m8yyj0z8neead6fbb\n",
            "\n",
            "ckz4gml9v90l80z9ihua0dcqf\n",
            "\n",
            "ckz4gpa6m92u5109bay4ud4q9\n",
            "\n",
            "ckz4gpaug92uf109ba7xfh8qc\n",
            "\n",
            "ckz4gqg2591kx0z9i627agz3h\n",
            "\n",
            "ckz4gr0bo90gr0z8nhmwg723b\n",
            "\n",
            "ckz4groex93gm109bhb2gbrya\n",
            "\n",
            "ckz4gs7ew90r50z8nagsw9zm8\n",
            "\n",
            "ckz4gsphf90um0z8n5mde91s2\n",
            "\n",
            "ckz4gt5wv90xr0z8n68jrdm67\n",
            "\n",
            "ckz4gtvv4913d0z8n0pz9a36v\n",
            "\n",
            "ckz4gufod92f90z9i926yam0d\n",
            "\n",
            "ckz4guvwc92hj0z9i78tqffku\n",
            "\n",
            "ckz4gvebz946b109b1prfcyj4\n",
            "\n",
            "ckz4gwjny94de109b3fwkdh12\n",
            "\n",
            "ckz4gxcud92yy0z9ia30kfxas\n",
            "\n",
            "ckz4gxyd8929h0z8naal761m5\n",
            "\n",
            "ckz4gylkw93g30z9i3y100a6j\n",
            "\n",
            "ckz4gz6g892lm0z8n6x6r9hoc\n",
            "\n",
            "ckz4gzp6393oc0z9igapz3fkn\n",
            "\n",
            "ckz4h0wwc92vh0z8naxsd2cv1\n",
            "\n",
            "ckz4h1cne92wj0z8n82kwa9ge\n",
            "\n",
            "ckz4h1vk592yh0z8na1hbbq55\n",
            "\n",
            "ckz4h2fba95it109b1xt28qm3\n",
            "\n",
            "ckz4h3fg895p5109bbz3f2rgs\n",
            "\n",
            "ckz4h4kqk94fi0z9ib6hccmjx\n",
            "\n",
            "ckz4h5ovs963k109b9i6y9nle\n",
            "\n",
            "ckz4h6db293pp0z8n61rw3jz0\n",
            "\n",
            "ckz4h6xf79688109b8sozf7jv\n",
            "\n",
            "ckz4h7jei93vq0z8n15zyhhjq\n",
            "\n",
            "ckz4h9r6h946y0z8ndxf78a0n\n",
            "\n",
            "ckz4h41gm95uy109b942ch81f\n",
            "\n",
            "ckz4h54bv93js0z8n0i0rd5yq\n",
            "\n",
            "ckz4h83su96gj109b0a424r30\n",
            "\n",
            "ckz4habrn95qn0z9ie94vakpe\n",
            "\n",
            "ckz4hb16295vt0z9icijndwl9\n",
            "\n",
            "ckz4hbj83970l109b104qf8od\n",
            "\n",
            "ckz4hc1889742109bcuurhg7k\n",
            "\n",
            "ckz4hcxnz94t70z8n4zfl9hez\n",
            "\n",
            "ckz4hdfq597el109b6eooehod\n",
            "\n",
            "ckz4he2e3952c0z8nbnjj6gaz\n",
            "\n",
            "ckz4heuwm958x0z8ng2i7c2d7\n",
            "\n",
            "ckz4hfb2l96qx0z9i2tlo3p2c\n",
            "\n",
            "ckz4hfvbs97x8109b8yly63rs\n",
            "\n",
            "ckz4hgbuc95io0z8n6fp75rkj\n",
            "\n",
            "ckz4hh3869872109b5p3vfzqt\n",
            "\n",
            "ckz4hhudz978c0z9i0jli11uq\n",
            "\n",
            "ckz4hitgq97f10z9i6wqja0tz\n",
            "\n",
            "ckz4hk4fo97pn0z9i6b3ubf0y\n",
            "\n",
            "ckz4mu6x90odt0z6ag8yrgev5\n",
            "\n",
            "ckz4mu61j0pak0z8780td8ic9\n",
            "\n",
            "ckz4mvbmp0oi50z6a9nx7fcz5\n",
            "\n",
            "ckz4mw3vt0pb70z8ze1rl0a3r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDlb9bIQmtWe"
      },
      "source": [
        "### Mapa de Saliência\n"
      ],
      "id": "zDlb9bIQmtWe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3LWqrkGqeA1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import  torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from matplotlib import pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "#define transforms to preprocess input image into format expected by model\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "#inverse transform to get normalize image back to original form for visualization\n",
        "inv_normalize = transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]\n",
        ")\n",
        "\n",
        "#transforms to resize image to the size expected by pretrained model,\n",
        "#convert PIL image to tensor, and\n",
        "#normalize the image\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    #normalize,\n",
        "])\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "G3LWqrkGqeA1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "999c1ade"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def saliency(img, model):\n",
        "    #we don't need gradients w.r.t. weights for a trained model\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    #set model in eval model\n",
        "    model.eval()\n",
        "\n",
        "    #transoform input PIL image to torch.Tensor and normalize\n",
        "    input = transform(img)\n",
        "    input.unsqueeze_(0)\n",
        "\n",
        "    #we want to calculate gradient of higest score w.r.t. input\n",
        "    #so set requires_grad to True for input\n",
        "    input.requires_grad = True\n",
        "\n",
        "\n",
        "    #forward pass to calculate predictions\n",
        "    preds = model(input)\n",
        "\n",
        "    score, indices = torch.max((preds[0]['masks']), 1)\n",
        "\n",
        "\n",
        "\n",
        "    #-------colocado pela Ju\n",
        "    score.mean().backward()\n",
        "    #print(score.sum())\n",
        "\n",
        "\n",
        "    #get max along channel axis\n",
        "    slc, _ = torch.max(torch.abs(input.grad[0]), dim=0)\n",
        "\n",
        "    slc = (slc - slc.min())/(slc.max()-slc.min())\n",
        "    #apply inverse transform on image\n",
        "\n",
        "    #usa a normalização - tirei porque talvez estivesse atrapalhando o modelo do Fabrício\n",
        "    '''with torch.no_grad():\n",
        "        input_img = inv_normalize(input[0])'''\n",
        "\n",
        "    masks = preds[0]['masks'][0].squeeze().detach().cpu().numpy()\n",
        "    masks_uint8 = 255 * np.uint8(masks > 0.5)\n",
        "    masks_float = np.float32(masks_uint8 > 0.5)\n",
        "\n",
        "    copia_img =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "    mascara_merge = np.zeros((copia_img.shape[0], copia_img.shape[1]))\n",
        "\n",
        "    for p in preds[0]['masks']:\n",
        "      t = p.squeeze().detach().cpu().numpy()\n",
        "      for i in range(len(t)):\n",
        "        for j in range(len(t[i])):\n",
        "            if t[i][j]>0.5:\n",
        "              mascara_merge[i][j] = 1\n",
        "\n",
        "    return slc"
      ],
      "id": "999c1ade"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05dc8ada"
      },
      "source": [
        "### Grad-CAM"
      ],
      "id": "05dc8ada"
    },
    {
      "cell_type": "code",
      "source": [
        "# from prediction_utils import URLDataset, get_maskrcnn_model, get_transform, display_pred, display_black_mask, display_white_mask, get_coloured_mask, check_resolutions\n",
        "from torchvision.transforms import ToPILImage\n",
        "from matplotlib import pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n"
      ],
      "metadata": {
        "id": "h72peMzsgza5"
      },
      "id": "h72peMzsgza5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SegmentationModelOutputWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(SegmentationModelOutputWrapper, self).__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model([F.to_tensor(x)])[0][\"masks\"]"
      ],
      "metadata": {
        "id": "ZJL3Q05FiQCM"
      },
      "id": "ZJL3Q05FiQCM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM\n",
        "import random\n",
        "\n",
        "class SemanticSegmentationTarget:\n",
        "    def __init__(self, category, mask):\n",
        "        self.category = category\n",
        "        self.mask = torch.from_numpy(mask)\n",
        "\n",
        "    def __call__(self, model_output):\n",
        "        return (model_output['masks'][self.category, :, : ] * self.mask).sum()"
      ],
      "metadata": {
        "id": "aTsgEc2ji1C8"
      },
      "id": "aTsgEc2ji1C8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zudBpiTRNGMr"
      },
      "source": [
        "###CNN Filters\n",
        "\n",
        "https://ravivaishnav20.medium.com/visualizing-feature-maps-using-pytorch-12a48cd1e573"
      ],
      "id": "zudBpiTRNGMr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXGEimL_NFLx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models, transforms, utils\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "import json\n",
        "%matplotlib inline"
      ],
      "id": "CXGEimL_NFLx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4uJRcd3PWSQ"
      },
      "outputs": [],
      "source": [
        "def CnnFilters(imagem, model):\n",
        "\n",
        "\n",
        "  model_weights =[]\n",
        "\n",
        "  conv_layers = []\n",
        "\n",
        "  model_children = list(model.backbone)\n",
        "\n",
        "  counter = 0\n",
        "  #append all the conv layers and their respective wights to the list\n",
        "  for i in range(len(model_children)):\n",
        "      if type(model_children[i]) == nn.Conv2d:\n",
        "          counter+=1\n",
        "          model_weights.append(model_children[i].weight)\n",
        "          conv_layers.append(model_children[i])\n",
        "      elif type(model_children[i]) == nn.Sequential:\n",
        "          for j in range(len(model_children[i])):\n",
        "              for child in model_children[i][j].children():\n",
        "                  if type(child) == nn.Conv2d:\n",
        "                      counter+=1\n",
        "                      model_weights.append(child.weight)\n",
        "                      conv_layers.append(child)\n",
        "\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = model.to(device)\n",
        "\n",
        "  outputs = []\n",
        "  names = []\n",
        "  for layer in conv_layers[0:]:\n",
        "      imagem = layer(imagem)\n",
        "      outputs.append(imagem)\n",
        "      names.append(str(layer))\n",
        "\n",
        "\n",
        "  processed = []\n",
        "  for feature_map in outputs:\n",
        "      feature_map = feature_map.squeeze(0)\n",
        "      gray_scale = torch.sum(feature_map,0)\n",
        "      gray_scale = gray_scale / feature_map.shape[0]\n",
        "      processed.append(gray_scale.data.cpu().numpy())\n",
        "\n",
        "  result = outputs[0][0]\n",
        "  return result"
      ],
      "id": "P4uJRcd3PWSQ"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FXDu-ZGdnuKO"
      },
      "id": "FXDu-ZGdnuKO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Layer GradCAM"
      ],
      "metadata": {
        "id": "O52jfbAJnuUA"
      },
      "id": "O52jfbAJnuUA"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VZt-D8EnyIj",
        "outputId": "2d3f8c2f-df34-456d-adb2-d22db885a1b3"
      },
      "id": "4VZt-D8EnyIj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.5)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from prediction_utils import URLDataset, get_maskrcnn_model, get_transform, display_pred, display_black_mask, display_white_mask, get_coloured_mask, check_resolutions\n",
        "from torchvision.transforms import ToPILImage\n",
        "from matplotlib import pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "from captum.attr import visualization as viz\n",
        "from captum.attr import LayerGradCam, FeatureAblation, LayerActivation, LayerAttribution"
      ],
      "metadata": {
        "id": "lAiStNaGol8Z"
      },
      "id": "lAiStNaGol8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_maskrcnn_model(num_classes=4, FROZEN_BLOCKS=1)\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/XAI/resnet18_checkpoint_355.pth', map_location=torch.device(\"cpu\")))\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device).eval()\n"
      ],
      "metadata": {
        "id": "OBnh18mTtn-v"
      },
      "id": "OBnh18mTtn-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This wrapper computes the segmentation model output and sums the pixel scores for\n",
        "all pixels predicted as each class, returning a tensor with a single value for\n",
        "each class. This makes it easier to attribute with respect to a single output\n",
        "scalar, as opposed to an individual pixel output attribution.\n",
        "\"\"\"\n",
        "def agg_segmentation_wrapper(inp):\n",
        "    model_out = model(inp)[0]\n",
        "    #print(model_out)\n",
        "    # Creates binary matrix with 1 for original argmax class for each pixel\n",
        "    # and 0 otherwise. Note that this may change when the input is ablated\n",
        "    # so we use the original argmax predicted above, out_max.\n",
        "    out_max = torch.argmax(model_out[\"masks\"], dim=1, keepdim=True)\n",
        "    selected_inds = torch.zeros_like(model_out[\"masks\"]).scatter_(1, out_max, 1)\n",
        "    return (model_out[\"masks\"] * selected_inds).sum(dim=(2,3))\n",
        "\n",
        "    #Original\n",
        "    # return (model_out[\"masks\"][0] * selected_inds).sum(dim=(2,3))\n",
        "\n",
        "\n",
        "# Alternate wrapper, simply summing each output channel\n",
        "def wrapper(inp):\n",
        "    return model(inp)[0]['masks'].sum(dim=(2,3))"
      ],
      "metadata": {
        "id": "nupKt8IZo1nL"
      },
      "id": "nupKt8IZo1nL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def decode_segmap(image, nc=21, threshold=0.5):\n",
        "\n",
        "    label_colors = np.array([(0, 0, 0),  # 0=background\n",
        "               # 1=cabeça, 2=nadadeiras, 3=corpo, 4=?\n",
        "               (128, 0, 0), (0, 128, 0), (128, 128, 0), (128, 128, 0)])\n",
        "\n",
        "\n",
        "    r = np.zeros_like(image).astype(np.uint8)\n",
        "    g = np.zeros_like(image).astype(np.uint8)\n",
        "    b = np.zeros_like(image).astype(np.uint8)\n",
        "\n",
        "    for l in range(0, nc):\n",
        "        idx = image > threshold\n",
        "        r[idx] = label_colors[l, 0]\n",
        "        g[idx] = label_colors[l, 1]\n",
        "        b[idx] = label_colors[l, 2]\n",
        "\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    return rgb"
      ],
      "metadata": {
        "id": "B93VfVd5o2lg"
      },
      "id": "B93VfVd5o2lg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculos\n"
      ],
      "metadata": {
        "id": "afqzZIddNdAg"
      },
      "id": "afqzZIddNdAg"
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def CalcularIndices(copia_img, mask_cam, nome, cont, i):\n",
        "\n",
        "\n",
        "\n",
        "      Image.fromarray(np.uint8(copia_img)).save(\"/content/drive/MyDrive/Colab Notebooks/XAI/MapadeSaliencia_LayerGradCAM/Aleatorio/peixe\"+str(cont)+\"_mapa_layer_aleatorio\"+str(i)+\".png\")\n",
        "\n",
        "      # print(\"peixe\"+str(cont)+\"_gradcam_preto\"+str(i)+\".png\")\n",
        "\n",
        "      img_mask = Image.open(\"/content/drive/MyDrive/Colab Notebooks/XAI/Máscaras peixes/\"+nome+\".png\")\n",
        "      mascara_original = np.asarray(img_mask)\n",
        "\n",
        "      #intersecção sobre a união\n",
        "      a = np.uint8(mask_cam)\n",
        "      b = np.uint8(mascara_original)\n",
        "\n",
        "\n",
        "      mask1_area = np.count_nonzero(a == 1)\n",
        "      mask2_area = np.count_nonzero(b == 1)\n",
        "      intersection = np.count_nonzero(np.logical_and(a==1,  b==1))\n",
        "      iou = intersection/(mask1_area+mask2_area-intersection)\n",
        "\n",
        "      # print(iou)\n",
        "\n",
        "\n",
        "      #sorense dice\n",
        "      dice = np.sum(mask_cam[mascara_original==1])*2.0 / (np.sum(mask_cam) + np.sum(mascara_original))\n",
        "\n",
        "      # print(dice)\n",
        "\n",
        "\n",
        "      with open(\"/content/drive/MyDrive/Colab Notebooks/XAI/MapadeSaliencia_LayerGradCAM/Aleatorio/indices_mapa_layer_aleatorio.csv\", \"a\", newline=\"\") as csvfile:\n",
        "        spamwriter = csv.writer(csvfile, delimiter=\";\", quotechar=\"|\", quoting=csv.QUOTE_MINIMAL, lineterminator='\\n')\n",
        "        spamwriter.writerow([cont, nome, str(iou), str(dice), i])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cuTqeP1Q6uSz"
      },
      "id": "cuTqeP1Q6uSz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from prediction_utils import URLDataset, get_maskrcnn_model, get_transform, display_pred, display_black_mask, display_white_mask, get_coloured_mask, check_resolutions\n",
        "from torchvision.transforms import ToPILImage\n",
        "from matplotlib import pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "def calc_gradcam(image):\n",
        "\n",
        "\n",
        "      output2 = model([F.to_tensor(image)])[0]\n",
        "\n",
        "\n",
        "      segmodel = SegmentationModelOutputWrapper(model)\n",
        "      output = segmodel(image)\n",
        "      output[0].shape\n",
        "\n",
        "\n",
        "      #máscaras da predição\n",
        "\n",
        "      # output = segmodel(image)\n",
        "      normalized_masks = torch.nn.functional.softmax(output, dim=1).cpu()\n",
        "\n",
        "      sem_classes = [\n",
        "          'nadadeiras','corpo', 'cabeca'\n",
        "      ]\n",
        "      sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
        "\n",
        "      copia_img1 =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "      mascaras_todas = np.zeros((copia_img1.shape[0], copia_img1.shape[1]))\n",
        "\n",
        "      for p in output2['masks']:\n",
        "        t = p.squeeze().detach().cpu().numpy()\n",
        "        for i in range(len(t)):\n",
        "          for j in range(len(t[i])):\n",
        "              if t[i][j]>0.5:\n",
        "                mascaras_todas[i][j] = 1\n",
        "\n",
        "\n",
        "\n",
        "      car_mask = torch.from_numpy(mascaras_todas)\n",
        "      mask = car_mask.clone().detach().numpy()\n",
        "\n",
        "      #aplicacao grad-cam\n",
        "\n",
        "      target_layers = [model.backbone[7]]\n",
        "\n",
        "      targets = [SemanticSegmentationTarget(None, mascaras_todas)]\n",
        "\n",
        "\n",
        "\n",
        "      with GradCAM(model=model, target_layers=target_layers) as cam:\n",
        "          grayscale_cam = cam(input_tensor=F.to_tensor(image).unsqueeze(dim=0),targets=targets)[0, :]\n",
        "          rgb_img = np.float32(image.convert(\"RGB\")) / 255\n",
        "\n",
        "\n",
        "          grayscale_cam = np.moveaxis(grayscale_cam, 0, 0)\n",
        "\n",
        "          cam_image = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
        "\n",
        "\n",
        "          # #Pertubação de pixels\n",
        "          copia_img1 =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "          # #Matriz zerada da imagem\n",
        "          mask_cam2 = np.zeros((copia_img1.shape[0], copia_img1.shape[1]))\n",
        "\n",
        "          for i in range(len(grayscale_cam)):\n",
        "            for j in range(len(grayscale_cam[i])):\n",
        "              if grayscale_cam[i][j]>0.01:\n",
        "                mask_cam2 [i][j] = 1\n",
        "\n",
        "          return mask_cam2"
      ],
      "metadata": {
        "id": "yJ7iKOmVN21G"
      },
      "id": "yJ7iKOmVN21G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_mapa(image):\n",
        "\n",
        "      model = get_maskrcnn_model(num_classes=4, FROZEN_BLOCKS=1)\n",
        "\n",
        "\n",
        "      model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/XAI/resnet18_checkpoint_355.pth', map_location=torch.device(\"cpu\")))\n",
        "\n",
        "\n",
        "      slc = saliency(image, model)\n",
        "\n",
        "\n",
        "      copia_img2 =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "      #Matriz zerada da imagem\n",
        "      mask_cam = np.zeros((copia_img2.shape[0], copia_img2.shape[1]))\n",
        "\n",
        "      for i in range(len(slc)):\n",
        "        for j in range(len(slc[i])):\n",
        "          if slc[i][j]>0.045:\n",
        "              mask_cam [i][j] = 1\n",
        "\n",
        "\n",
        "\n",
        "      return mask_cam"
      ],
      "metadata": {
        "id": "iv-uCzsbOHNy"
      },
      "id": "iv-uCzsbOHNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "def calc_cnnfilters(image):\n",
        "\n",
        "    model = get_maskrcnn_model(num_classes=4, FROZEN_BLOCKS=1)\n",
        "\n",
        "    model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/XAI/resnet18_checkpoint_355.pth', map_location=torch.device(\"cpu\")))\n",
        "\n",
        "    model = model.eval()\n",
        "\n",
        "\n",
        "    preprocessing = transforms.Compose([transforms.ToTensor()])\n",
        "    preproc_img = preprocessing(image)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=0., std=1.)\n",
        "    ])\n",
        "    imagem = transform(image)\n",
        "\n",
        "\n",
        "    normalized_inp = preproc_img.unsqueeze(0).to('cpu')\n",
        "    normalized_inp.requires_grad = True\n",
        "    out = model(normalized_inp)\n",
        "\n",
        "\n",
        "    result =  CnnFilters(imagem, model)\n",
        "\n",
        "\n",
        "    #preparo perturbação\n",
        "\n",
        "    copia_result = np.zeros((result.shape[0], result.shape[1], 3))\n",
        "\n",
        "    for i in range(len(result)):\n",
        "      for j in range(len(result[i])):\n",
        "        if result[i][j]<0:\n",
        "          copia_result [i][j][0] = 255\n",
        "          copia_result [i][j][1] = 0\n",
        "          copia_result [i][j][2] = 0\n",
        "\n",
        "    resultado = cv2.resize(copia_result, (image.size[0], image.size[1]), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "    copia_img =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "    #Matriz zerada da imagem\n",
        "    mask_cam3 = np.zeros((copia_img.shape[0], copia_img.shape[1]))\n",
        "\n",
        "    for i in range(len(resultado)):\n",
        "      for j in range(len(resultado[i])):\n",
        "        if resultado[i][j][0]>128:\n",
        "          mask_cam3 [i][j] = 1\n",
        "\n",
        "    return mask_cam3\n"
      ],
      "metadata": {
        "id": "tM74Skmi3bCd"
      },
      "id": "tM74Skmi3bCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_layergradcam(img):\n",
        "\n",
        "\n",
        "      preprocessing = transforms.Compose([transforms.ToTensor()])\n",
        "      preproc_img = preprocessing(img)\n",
        "\n",
        "      normalized_inp = preproc_img.unsqueeze(0).to(device)\n",
        "      normalized_inp.requires_grad = True\n",
        "\n",
        "      out = model(normalized_inp)\n",
        "\n",
        "      #Feito pela Ju - tensor estava voltando zerado\n",
        "      out_mask = out[0]['masks']\n",
        "      out_labels = out[0]['labels']\n",
        "\n",
        "\n",
        "      out_max = torch.argmax(out_mask, dim=1, keepdim=True)\n",
        "\n",
        "      for mask in out_mask:\n",
        "        rgb = decode_segmap(mask.detach().cpu().squeeze().numpy(), 3, 0.90)\n",
        "\n",
        "\n",
        "      lgc = LayerGradCam(agg_segmentation_wrapper, model.backbone[7])\n",
        "      gc_attr = lgc.attribute(normalized_inp, target=0)\n",
        "\n",
        "      # la = LayerActivation(wrapper,  model.backbone[7])\n",
        "      # activation = la.attribute(normalized_inp)\n",
        "      result = gc_attr[0].cpu().permute(1,2,0).detach().numpy()\n",
        "      result2 = gc_attr[0].cpu().permute(1,2,0).detach().numpy()\n",
        "      copia_result2 = np.zeros((result2.shape[0], result2.shape[1], 3))\n",
        "\n",
        "      for k in range(len(result2)):\n",
        "        for l in range(len(result2[k])):\n",
        "          if result2[k][l]>-0.5:\n",
        "            copia_result2 [k][l][0] = 255\n",
        "            copia_result2 [k][l][1] = 0\n",
        "            copia_result2 [k][l][2] = 0\n",
        "\n",
        "      resultado2 = cv2.resize(copia_result2, (img.size[0],img.size[1]), interpolation = cv2.INTER_LINEAR)\n",
        "\n",
        "\n",
        "      #Perturbação de pixels\n",
        "      copia_img2 =  np.float32(img.convert(\"RGB\"))\n",
        "\n",
        "      #Matriz zerada da imagem\n",
        "      mask_cam2 = np.zeros((copia_img2.shape[0], copia_img2.shape[1]))\n",
        "\n",
        "      # #RGB - Preto\n",
        "      for i in range(len(resultado2)):\n",
        "       for j in range(len(resultado2[i])):\n",
        "        if resultado2[i][j][0]>128:\n",
        "             mask_cam2 [i][j] = 1\n",
        "             # print(grayscale_cam[i][j])\n",
        "\n",
        "      return mask_cam2"
      ],
      "metadata": {
        "id": "jrgn01Ckn3tC"
      },
      "id": "jrgn01Ckn3tC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2d6b00b",
        "outputId": "64faaa4e-ada9-4c7f-b21f-ec5dc044cf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ckz4dow5r8eay0z9i9oef91zg\n",
            "1\n",
            "2\n",
            "ckz4e0jvq8h73109bbpqs0fps\n",
            "1\n",
            "2\n",
            "ckz4e2n4q8f530z8n9s3ubp7y\n",
            "1\n",
            "2\n",
            "ckz4e3onq8gmi0z9i5c70ay45\n",
            "1\n",
            "2\n",
            "ckz4e4vvo8i6i109bgzla6hms\n",
            "1\n",
            "2\n",
            "ckz4e8ge48hhl0z9ifd9773xk\n",
            "1\n",
            "2\n",
            "ckz4e9ebq8gg20z8ngztba1ez\n",
            "1\n",
            "2\n",
            "ckz4e650k8ig1109b97k528nn\n",
            "1\n",
            "2\n",
            "ckz4e72918ion109b5wbr5raw\n",
            "1\n",
            "2\n",
            "ckz4ea3cz8jbf109bhn9y68q1\n",
            "1\n",
            "2\n",
            "ckz4ebk6g8i8u0z9i60n60f6l\n",
            "1\n",
            "2\n",
            "ckz4edumg8ing0z9ifni9g5t8\n",
            "1\n",
            "2\n",
            "ckz4efrf38kem109b2057fb9a\n",
            "1\n",
            "2\n",
            "ckz4egfr48kh7109bah7xf5az\n",
            "1\n",
            "2\n",
            "ckz4fduq78ntw0z8n630rag87\n",
            "1\n",
            "2\n",
            "ckz4ff6t28o3e0z8neshr668d\n",
            "1\n",
            "2\n",
            "ckz4ffyse8o9s0z8n3zgmd73e\n",
            "1\n",
            "2\n",
            "ckz4fhxu88ool0z8n6rwr1cpm\n",
            "1\n",
            "2\n",
            "ckz4fiupe8rhx109bh3peeyhc\n",
            "1\n",
            "2\n",
            "ckz4fzcn28sui0z8nakiacny4\n",
            "1\n",
            "2\n",
            "ckz4g2msm8tqx0z8n6d9pb3iv\n",
            "1\n",
            "2\n",
            "ckz4g4h098uag0z8nht9q26i8\n",
            "1\n",
            "2\n",
            "ckz4g5tsc8um20z8n9i7a5xw6\n",
            "1\n",
            "2\n",
            "ckz4g6h9v8x8e109b16pefhfo\n",
            "1\n",
            "2\n",
            "ckz4g7nl08v2x0z8nbuek2vjk\n",
            "1\n",
            "2\n",
            "ckz4g09s18vbm109bhm9pcd53\n",
            "1\n",
            "2\n",
            "ckz4g9uib8x7y0z9i2x6dgzf8\n",
            "1\n",
            "2\n",
            "ckz4g17ra8upt0z9ibj0bbdno\n",
            "1\n",
            "2\n",
            "ckz4g51wk8wtj109bhzrydupk\n",
            "1\n",
            "2\n",
            "ckz4g70yq8we70z9idki606ak\n",
            "1\n",
            "2\n",
            "ckz4g39038vcp0z9i67o96efj\n",
            "1\n",
            "2\n",
            "ckz4g88978v9h0z8n0bmhf6m2\n",
            "1\n",
            "2\n",
            "ckz4gb3qw8yne109bd7k39lcl\n",
            "1\n",
            "2\n",
            "ckz4gbrb08xqo0z9i6tencxws\n",
            "1\n",
            "2\n",
            "ckz4gceez8xwm0z9ihjmr0f68\n",
            "1\n",
            "2\n",
            "ckz4gd9eh8y3f0z9i649mbt2l\n",
            "1\n",
            "2\n",
            "ckz4gds8l8zdg109b2j93eman\n",
            "1\n",
            "2\n",
            "3\n",
            "ckz4gedyg8yf60z9i2yz18o5f\n",
            "1\n",
            "2\n",
            "ckz4gf4eo8zsy109b73nq9xh9\n",
            "1\n",
            "2\n",
            "ckz4gfy7f909n109b48iz9b6o\n",
            "1\n",
            "2\n",
            "ckz4ggtun8z5p0z9i0l5j7dsa\n",
            "1\n",
            "2\n",
            "ckz4ghiqq8zco0z9i22va9vso\n",
            "1\n",
            "2\n",
            "ckz4gi8fr90wz109b4554e8e3\n",
            "1\n",
            "2\n",
            "ckz4gity68yeu0z8n2tl4bvke\n",
            "1\n",
            "2\n",
            "ckz4gjkpy8zwc0z9ihsd41tus\n",
            "1\n",
            "2\n",
            "ckz4gk5d08ypo0z8n6p8eear8\n",
            "1\n",
            "2\n",
            "ckz4gksld91kk109b5t1y78vp\n",
            "1\n",
            "2\n",
            "ckz4glf1m8yyj0z8neead6fbb\n",
            "1\n",
            "2\n",
            "ckz4gml9v90l80z9ihua0dcqf\n",
            "1\n",
            "2\n",
            "ckz4gpa6m92u5109bay4ud4q9\n",
            "1\n",
            "2\n",
            "ckz4gpaug92uf109ba7xfh8qc\n",
            "1\n",
            "2\n",
            "ckz4gqg2591kx0z9i627agz3h\n",
            "1\n",
            "2\n",
            "ckz4gr0bo90gr0z8nhmwg723b\n",
            "1\n",
            "2\n",
            "ckz4groex93gm109bhb2gbrya\n",
            "1\n",
            "2\n",
            "ckz4gs7ew90r50z8nagsw9zm8\n",
            "1\n",
            "2\n",
            "ckz4gsphf90um0z8n5mde91s2\n",
            "1\n",
            "2\n",
            "ckz4gt5wv90xr0z8n68jrdm67\n",
            "1\n",
            "2\n",
            "ckz4gtvv4913d0z8n0pz9a36v\n",
            "1\n",
            "2\n",
            "ckz4gufod92f90z9i926yam0d\n",
            "1\n",
            "2\n",
            "ckz4guvwc92hj0z9i78tqffku\n",
            "1\n",
            "2\n",
            "ckz4gvebz946b109b1prfcyj4\n",
            "1\n",
            "2\n",
            "ckz4gwjny94de109b3fwkdh12\n",
            "1\n",
            "2\n",
            "ckz4gxcud92yy0z9ia30kfxas\n",
            "1\n",
            "2\n",
            "ckz4gxyd8929h0z8naal761m5\n",
            "1\n",
            "2\n",
            "ckz4gylkw93g30z9i3y100a6j\n",
            "1\n",
            "2\n",
            "ckz4gz6g892lm0z8n6x6r9hoc\n",
            "1\n",
            "2\n",
            "ckz4gzp6393oc0z9igapz3fkn\n",
            "1\n",
            "2\n",
            "ckz4h0wwc92vh0z8naxsd2cv1\n",
            "1\n",
            "2\n",
            "ckz4h1cne92wj0z8n82kwa9ge\n",
            "1\n",
            "2\n",
            "ckz4h1vk592yh0z8na1hbbq55\n",
            "1\n",
            "2\n",
            "ckz4h2fba95it109b1xt28qm3\n",
            "1\n",
            "2\n",
            "ckz4h3fg895p5109bbz3f2rgs\n",
            "1\n",
            "2\n",
            "ckz4h4kqk94fi0z9ib6hccmjx\n",
            "1\n",
            "2\n",
            "ckz4h5ovs963k109b9i6y9nle\n",
            "1\n",
            "2\n",
            "ckz4h6db293pp0z8n61rw3jz0\n",
            "1\n",
            "2\n",
            "ckz4h6xf79688109b8sozf7jv\n",
            "1\n",
            "2\n",
            "ckz4h7jei93vq0z8n15zyhhjq\n",
            "1\n",
            "2\n",
            "ckz4h9r6h946y0z8ndxf78a0n\n",
            "1\n",
            "2\n",
            "ckz4h41gm95uy109b942ch81f\n",
            "1\n",
            "2\n",
            "ckz4h54bv93js0z8n0i0rd5yq\n",
            "1\n",
            "2\n",
            "ckz4h83su96gj109b0a424r30\n",
            "1\n",
            "2\n",
            "ckz4habrn95qn0z9ie94vakpe\n",
            "1\n",
            "2\n",
            "ckz4hb16295vt0z9icijndwl9\n",
            "1\n",
            "2\n",
            "ckz4hbj83970l109b104qf8od\n",
            "1\n",
            "2\n",
            "ckz4hc1889742109bcuurhg7k\n",
            "1\n",
            "2\n",
            "ckz4hcxnz94t70z8n4zfl9hez\n",
            "1\n",
            "2\n",
            "ckz4hdfq597el109b6eooehod\n",
            "1\n",
            "2\n",
            "ckz4he2e3952c0z8nbnjj6gaz\n",
            "1\n",
            "2\n",
            "ckz4heuwm958x0z8ng2i7c2d7\n",
            "1\n",
            "2\n",
            "ckz4hfb2l96qx0z9i2tlo3p2c\n",
            "1\n",
            "2\n",
            "ckz4hfvbs97x8109b8yly63rs\n",
            "1\n",
            "2\n",
            "ckz4hgbuc95io0z8n6fp75rkj\n",
            "1\n",
            "2\n",
            "ckz4hh3869872109b5p3vfzqt\n",
            "1\n",
            "2\n",
            "ckz4hhudz978c0z9i0jli11uq\n",
            "1\n",
            "2\n",
            "ckz4hitgq97f10z9i6wqja0tz\n",
            "1\n",
            "2\n",
            "ckz4hk4fo97pn0z9i6b3ubf0y\n",
            "1\n",
            "2\n",
            "ckz4mu6x90odt0z6ag8yrgev5\n",
            "1\n",
            "2\n",
            "ckz4mu61j0pak0z8780td8ic9\n",
            "1\n",
            "2\n",
            "ckz4mvbmp0oi50z6a9nx7fcz5\n",
            "1\n",
            "2\n",
            "ckz4mw3vt0pb70z8ze1rl0a3r\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "cont = 0\n",
        "\n",
        "for imagens in linhas:\n",
        "  cont += 1\n",
        "  nome = imagens[0:25]\n",
        "  print(nome)\n",
        "  image = Image.open(\"/content/drive/MyDrive/Colab Notebooks/XAI/Imagens peixes/\"+nome+\".png\").convert(\"RGB\")\n",
        "  # image = Image.open(\"/content/drive/MyDrive/Colab Notebooks/XAI/Imagens peixes/.png\").convert(\"RGB\")\n",
        "\n",
        "  for a in [1,2,3,4,5]:\n",
        "    try:\n",
        "      print(a)\n",
        "      if a>1:\n",
        "        num = a-1\n",
        "        image = Image.open(\"/content/drive/MyDrive/Colab Notebooks/XAI/MapadeSaliencia_LayerGradCAM/Aleatorio/peixe\"+str(cont)+\"_mapa_layer_aleatorio\"+str(num)+\".png\").convert(\"RGB\")\n",
        "\n",
        "      # mask1 = calc_gradcam(image)\n",
        "      # mask1 = calc_cnnfilters(image)\n",
        "      mask1 = calc_layergradcam(image)\n",
        "      mask2 = calc_mapa(image)\n",
        "\n",
        "\n",
        "      mascara_combinada = cv2.addWeighted(mask1, 1, mask2, 1, 0)\n",
        "      # m = Image.fromarray(np.uint8(mascara_combinada)*255).convert('RGB')\n",
        "      # m.show()\n",
        "      # #RGB - Preto\n",
        "\n",
        "      #Pertubação de pixels\n",
        "      copia_img =  np.float32(image.convert(\"RGB\"))\n",
        "\n",
        "\n",
        "      # Preto\n",
        "      # for i in range(len(mascara_combinada)):\n",
        "      #   for j in range(len(mascara_combinada[i])):\n",
        "      #     if mascara_combinada[i][j]!=0:\n",
        "      #         copia_img [i][j][0] = 0\n",
        "      #         copia_img [i][j][1] = 0\n",
        "      #         copia_img [i][j][2] = 0\n",
        "\n",
        "      # im = Image.fromarray(np.uint8(copia_img))\n",
        "      # im.show()\n",
        "\n",
        "      #White Noise\n",
        "\n",
        "      # for i in range(len(mascara_combinada)):\n",
        "      #   for j in range(len(mascara_combinada[i])):\n",
        "      #     if mascara_combinada[i][j]!=0:\n",
        "      #       rdn = random.random()\n",
        "      #       if rdn < 0.5:\n",
        "      #         copia_img [i][j][0] = 0\n",
        "      #         copia_img [i][j][1] = 0\n",
        "      #         copia_img [i][j][2] = 0\n",
        "      #       elif rdn > 0.5:\n",
        "      #         copia_img [i][j][0] = 255\n",
        "      #         copia_img [i][j][1] = 255\n",
        "      #         copia_img [i][j][2] = 255\n",
        "              # print(grayscale_cam[i][j])\n",
        "\n",
        "      #Pixels Aleatórios\n",
        "\n",
        "\n",
        "      for i in range(len(mascara_combinada)):\n",
        "        for j in range(len(mascara_combinada[i])):\n",
        "          if mascara_combinada[i][j]!=0:\n",
        "            copia_img [i][j][0] = random.randint(0, 255)\n",
        "            copia_img [i][j][1] = random.randint(0, 255)\n",
        "            copia_img [i][j][2] = random.randint(0, 255)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      CalcularIndices(copia_img, mascara_combinada, nome, cont, a)\n",
        "\n",
        "    except:\n",
        "        break\n",
        "\n"
      ],
      "id": "b2d6b00b"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}